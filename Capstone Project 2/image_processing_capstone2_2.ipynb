{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook run using keras: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "print('Notebook run using keras:', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "import keras.models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change some parameters or model itself?\n",
    "\n",
    "The ResNet50 Model from the previous file had:\n",
    "\n",
    "- batch size of 32\n",
    "- learning rate of 0.001\n",
    "- no rescaling of the pixel components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "IMAGE_SIZE = (160, 160)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "data_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split = 0.2,\n",
    "    rescale=1./255.\n",
    ")\n",
    "\n",
    "train_ds = data_gen.flow_from_directory(\n",
    "    'dataset',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training', \n",
    ")\n",
    "\n",
    "val_ds = data_gen.flow_from_directory(\n",
    "    'dataset',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make similar model with slight modifications\n",
    "\n",
    "# make base model; it will be frozen\n",
    "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# put the trainable layers of the model\n",
    "x = keras.layers.Flatten()(base_model.output)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dense(16, activation='relu')(x)\n",
    "prediction_layer = keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)\n",
    "\n",
    "model = keras.models.Model(inputs=base_model.input, outputs=prediction_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adamax(lr=0.001),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 130s 3s/step - loss: 1.4413 - accuracy: 0.2676 - val_loss: 1.3722 - val_accuracy: 0.3013\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 122s 2s/step - loss: 1.3694 - accuracy: 0.3254 - val_loss: 1.3604 - val_accuracy: 0.3313\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 120s 2s/step - loss: 1.3426 - accuracy: 0.3275 - val_loss: 1.3174 - val_accuracy: 0.4125\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 123s 2s/step - loss: 1.3211 - accuracy: 0.3767 - val_loss: 1.2818 - val_accuracy: 0.4162\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 121s 2s/step - loss: 1.2790 - accuracy: 0.4312 - val_loss: 1.2671 - val_accuracy: 0.4288\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like using the rescaling component in the preprocessor does not improve the model's accuracy.\n",
    "\n",
    "The batch size will remain unchanged for simplicity. However, I will try different learning rate values and a bigger target size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3200 images belonging to 4 classes.\n",
      "Found 800 images belonging to 4 classes.\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "IMAGE_SIZE = (200, 200)\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 4\n",
    "\n",
    "data_gen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split = 0.2,\n",
    "    #rescale=1./255.\n",
    ")\n",
    "\n",
    "train_ds = data_gen.flow_from_directory(\n",
    "    'dataset',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='training', \n",
    ")\n",
    "\n",
    "val_ds = data_gen.flow_from_directory(\n",
    "    'dataset',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make base model; it will be frozen\n",
    "base_model_2 = ResNet50(weights='imagenet', include_top=False, input_shape=(*IMAGE_SIZE, 3))\n",
    "for layer in base_model_2.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "# put the trainable layers of the model\n",
    "x = keras.layers.Flatten()(base_model_2.output)\n",
    "x = keras.layers.Dense(64, activation='relu')(x)\n",
    "x = keras.layers.Dense(32, activation='relu')(x)\n",
    "x = keras.layers.Dense(16, activation='relu')(x)\n",
    "prediction_layer = keras.layers.Dense(NUM_CLASSES, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with `lr = 0.01`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Model(inputs=base_model_2.input, outputs=prediction_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adamax(lr=0.01),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 206s 4s/step - loss: 22.4180 - accuracy: 0.4271 - val_loss: 1.4538 - val_accuracy: 0.7638\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 200s 4s/step - loss: 0.9983 - accuracy: 0.8367 - val_loss: 1.0329 - val_accuracy: 0.8275\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 199s 4s/step - loss: 0.3867 - accuracy: 0.9150 - val_loss: 1.0858 - val_accuracy: 0.8450\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 199s 4s/step - loss: 0.2939 - accuracy: 0.9232 - val_loss: 0.8943 - val_accuracy: 0.8700\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 200s 4s/step - loss: 0.1311 - accuracy: 0.9647 - val_loss: 0.8275 - val_accuracy: 0.8537\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try again with `lr = 0.001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = keras.models.Model(inputs=base_model_2.input, outputs=prediction_layer)\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy',\n",
    "                  optimizer=keras.optimizers.Adamax(lr=0.001),\n",
    "                  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 198s 4s/step - loss: 0.2287 - accuracy: 0.9502 - val_loss: 0.8247 - val_accuracy: 0.8750\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 192s 4s/step - loss: 0.0792 - accuracy: 0.9814 - val_loss: 0.8522 - val_accuracy: 0.8763\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 191s 4s/step - loss: 0.0882 - accuracy: 0.9857 - val_loss: 0.8383 - val_accuracy: 0.8712\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 190s 4s/step - loss: 0.0296 - accuracy: 0.9915 - val_loss: 0.9014 - val_accuracy: 0.8687\n"
     ]
    }
   ],
   "source": [
    "# this cell was run after training the model with lr = 0.01\n",
    "history2 = model2.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is weirdly very high. Perhaps if we trying fitting this first..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "50/50 [==============================] - 202s 4s/step - loss: 2.5294 - accuracy: 0.5607 - val_loss: 0.5665 - val_accuracy: 0.8200\n",
      "Epoch 2/5\n",
      "50/50 [==============================] - 194s 4s/step - loss: 0.3724 - accuracy: 0.8955 - val_loss: 0.4591 - val_accuracy: 0.8462\n",
      "Epoch 3/5\n",
      "50/50 [==============================] - 194s 4s/step - loss: 0.2076 - accuracy: 0.9361 - val_loss: 0.4497 - val_accuracy: 0.8625\n",
      "Epoch 4/5\n",
      "50/50 [==============================] - 195s 4s/step - loss: 0.1087 - accuracy: 0.9663 - val_loss: 0.4129 - val_accuracy: 0.8662\n",
      "Epoch 5/5\n",
      "50/50 [==============================] - 198s 4s/step - loss: 0.0915 - accuracy: 0.9772 - val_loss: 0.4196 - val_accuracy: 0.8700\n"
     ]
    }
   ],
   "source": [
    "# this cell was run without training the model with lr = 0.01 first\n",
    "history2 = model2.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "50/50 [==============================] - 198s 4s/step - loss: 2.2601 - accuracy: 0.4570 - val_loss: 0.7229 - val_accuracy: 0.7675\n",
      "Epoch 2/8\n",
      "50/50 [==============================] - 192s 4s/step - loss: 0.5630 - accuracy: 0.8278 - val_loss: 0.5907 - val_accuracy: 0.8125\n",
      "Epoch 3/8\n",
      "50/50 [==============================] - 192s 4s/step - loss: 0.4107 - accuracy: 0.8799 - val_loss: 0.5677 - val_accuracy: 0.8350\n",
      "Epoch 4/8\n",
      "50/50 [==============================] - 193s 4s/step - loss: 0.2489 - accuracy: 0.9363 - val_loss: 0.5734 - val_accuracy: 0.8512\n",
      "Epoch 5/8\n",
      "50/50 [==============================] - 193s 4s/step - loss: 0.1946 - accuracy: 0.9427 - val_loss: 0.5256 - val_accuracy: 0.8612\n",
      "Epoch 6/8\n",
      "50/50 [==============================] - 194s 4s/step - loss: 0.1513 - accuracy: 0.9655 - val_loss: 0.5470 - val_accuracy: 0.8775\n",
      "Epoch 7/8\n",
      "50/50 [==============================] - 195s 4s/step - loss: 0.1310 - accuracy: 0.9688 - val_loss: 0.5532 - val_accuracy: 0.8662\n",
      "Epoch 8/8\n",
      "50/50 [==============================] - 194s 4s/step - loss: 0.0871 - accuracy: 0.9767 - val_loss: 0.5438 - val_accuracy: 0.8825\n"
     ]
    }
   ],
   "source": [
    "history2 = model2.fit(\n",
    "    train_ds,\n",
    "    steps_per_epoch=50,\n",
    "    validation_data=val_ds,\n",
    "    epochs=8,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_model_2/assets\n"
     ]
    }
   ],
   "source": [
    "model2.save('my_model_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
